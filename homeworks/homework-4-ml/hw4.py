# hw4.py

import os.path
import numpy

######################################
#
# FUNCTIONS YOU WILL NEED TO MODIFY:
#  - linreg_closed_form
#  - loss
#  - linreg_grad_desc
#  - random_fourier_features
#
######################################

def linreg_model_sample(Theta,model_X):
    if model_X.shape[1]==1:
        ## get a bunch of evenly spaced X values in the same range as the passed in data
        sampled_X = numpy.linspace(model_X.min(axis=0),model_X.max(axis=0),100)
        ## get the Y values for our sampled X values by taking the dot-product with the model
        ## Note: we're appending a column of all ones so we can do this with a single matrix-vector multiply
        sampled_Y = numpy.hstack([numpy.ones((sampled_X.shape[0],1)),sampled_X]).dot(Theta)
        return sampled_X, sampled_Y
    elif model_X.shape[1]==2:
        ## Unfortunately, plotting surfaces is a bit more complicated, first we need
        ## a set of points that covers the area we want to plot. numpy.meshgrid is a helper function
        ## that will create two NxN arrays that vary over both the X and Y range given.
        sampled_X, sampled_Y = numpy.meshgrid(model_X[:,0],model_X[:,1])
        ## We can't just do a simple matrix multiply here, because plot_surface(...) is going to expect NxN arrays like
        ## those generated by numpy.meshgrid(...). So here we're explicitly pulling out the components of Theta as
        ## scalars and multiplying them across each element in the X and Y arrays to get the value for Z
        sampled_Z = sampled_X*Theta[1]+sampled_Y*Theta[2]+Theta[0]
        return sampled_X, sampled_Y, sampled_Z

def plot_helper(data_X, data_Y, model_X=None, model_Y=None, model_Z=None):
    import matplotlib.pyplot
    ## 2D plotting
    ## data_X.shape[1] is the number of columns in data_X, just as data_X.shape[0] is the number of rows
    if data_X.shape[1]==1:
        fig1 = matplotlib.pyplot.figure() ## creates a new figure object that we can plot into
        fig1.gca().scatter(data_X,data_Y) ## creates a scatterplot with the given set of X and Y points
        ## If we were given a model, we need to plot that
        if not(model_X is None) and not(model_Y is None):
            ## Plot the data from the model
            ## Note: we're using plot(...) instead of scatter(...) because we want a smooth curve
            fig1.gca().plot(model_X,model_Y,color='r')
        ## The graph won't actually be displayed until we .show(...) it. You can swap this with savefig(...) if you
        ## instead want to save an image of the graph instead of displaying it. You can also use the interface to save an
        ## image after displaying it
        matplotlib.pyplot.show() #fig1.show()
    ## 3D plotting
    elif data_X.shape[1]==2:
        ## This import statement 'registers' the ability to do 3D projections/plotting with matplotlib
        from mpl_toolkits.mplot3d import Axes3D
        fig1 = matplotlib.pyplot.figure()
        ## The format for 3D scatter is similar to 2D; just add the third dimension to the argument list
        fig1.gca(projection='3d').scatter(data_X[:,0],data_X[:,1],data_Y)
        if not(model_X is None) and not(model_Y is None) and not(model_Z is None):
            ## Now, with our X, Y, and Z arrays (all NxN), we can use plot_surface(...) to create a nice 3D surface
            fig1.gca(projection='3d').plot_surface(model_X, model_Y, model_Z,linewidth=0.0,color=(1.0,0.2,0.2,0.75))
        matplotlib.pyplot.show() #fig1.show()
    else:
        ## Matplotlib does not yet have the capability to plot in 4D
        print('Data is not in 2 or 3 dimensions, cowardly refusing to plot! (data_X.shape == {})'.format(data_X.shape))

## Data loading utility function
def load_data(fname,directory='data'):
    data = numpy.loadtxt(os.path.join(directory,fname),delimiter=',')
    rows,cols = data.shape
    X_dim = cols-1
    Y_dim = 1
    return data[:,:-1].reshape(-1,X_dim), data[:,-1].reshape(-1,Y_dim)

def vis_linreg_model(train_X, train_Y, Theta):
    sample_X, sample_Y = linreg_model_sample(Theta,train_X)
    #NOTE: this won't work directly with 3D data. Write your own function, or modify this one
    #to generate plots for 2D-noisy-lin.txt or other 3D data.
    plot_helper(train_X, train_Y, sample_X, sample_Y)

def vis_linreg_model_3d(train_X, train_Y, Theta):
    sample_X, sample_Y, sample_Z = linreg_model_sample(Theta,train_X)
    plot_helper(train_X, train_Y, sample_X, sample_Y, sample_Z)

###################
# YOUR CODE BELOW #
###################
def linreg_closed_form(train_X, train_Y):
    '''
    Computes the optimal parameters for the given training data in closed form


    Args:
        train_X (N-by-D numpy array): Training data features as a matrix of row vectors (train_X[i][j] is the jth component of the ith example)
        train_Y (length N numpy array): The training data target as a length N vector


    Returns:
        A length D+1 numpy array with the optimal parameters
    '''
    # closed form solution - XtX-1*Xt
    xt = numpy.transpose(train_X)
    xt_x = numpy.dot(xt, train_X)
    xt_x_inverse = numpy.linalg.inv(xt_x)
    xt_x_inverse_xt = numpy.dot(xt_x_inverse, xt)
    Theta = numpy.dot(xt_x_inverse_xt, train_Y)
    return Theta

###################
# YOUR CODE BELOW #
###################
def loss(Theta, train_X, train_Y):
    '''
    Computes the squared loss for the given setting of the parameters given the training data


    Args:
        Theta (length D+1 numpy array): the parameters of the model
        train_X (N-by-D numpy array): Training data features as a matrix of row vectors (train_X[i][j] is the jth component of the ith example)
        train_Y (length N numpy array): The training data target as a length N vector


    Returns:
        The (scalar) loss for the given parameters and data.
    '''
    # loss = 1/2 *N * sum of ((x_transpose*theta - y) *2)
    N = train_X.shape[0]

    sse = 0.0
    for i in range(N):
        x = train_X[i]
        y = train_Y[i]
        # h = xt * theta
        y_hat = numpy.dot(numpy.transpose(x), Theta)
        square_error = (y_hat - y) ** 2
        sse = sse + square_error
    rv = sse / (2 * N)
    return rv

###################
# YOUR CODE BELOW #
###################
def linreg_grad_desc(initial_Theta, train_X, train_Y, alpha=0.05, num_iters=500, print_iters=True):
    '''
    Fits parameters using gradient descent


    Args:
        initial_Theta ((D+1)-by-1 numpy array): The initial value for the parameters we're optimizing over
        train_X (N-by-D numpy array): Training data features as a matrix of row vectors (train_X[i][j] is the jth component of the ith example)
        train_Y (N-by-1 numpy array): The training data target as a vector
        alpha (float): the learning rate/step size, defaults to 0.05
        num_iters (int): number of iterations to run gradient descent for, defaults to 500


    Returns:
        The history of theta's and their associated loss as a list of tuples [ (Theta1,loss1), (Theta2,loss2), ...]
    '''
    cur_Theta = initial_Theta
    step_history = list()
    for k in range(1,num_iters+1):
        cur_loss = loss(cur_Theta, train_X, train_Y)
        step_history.append((cur_Theta, cur_loss))
        if print_iters:
            print("Iteration: {} , Loss: {} , Theta: {}".format(k,cur_loss,cur_Theta))
        #TODO: Add update equation here
        # gradient  = 1/N * ( (xt * x * theta) - xt*y) )

        # should move the two lines and xt_y outside the loop for better perf
        xt = numpy.transpose(train_X)
        xt_x = numpy.dot(xt, train_X)
        xt_x_theta = numpy.dot(xt_x, cur_Theta)
        xt_y = numpy.dot(xt, train_Y)
        gradient = (xt_x_theta - xt_y)/train_X.shape[0]

        # gradient descent equation - new_theta =  (previous theta) - alpha * (gradient)
        prev_theta = cur_Theta
        cur_Theta = prev_theta - (alpha * gradient)
    return step_history

def apply_RFF_transform(X,Omega,B):
    '''
    Transforms features into a Fourier basis with given samples

        Given a set of random inner products and translations, transform X into the Fourier basis, Phi(X)
            phi_k(x) = cos(<x,omega_k> + b_k)                           #scalar form
            Phi(x) = sqrt(1/D)*[phi_1(x), phi_2(x), ..., phi_NFF(x)].T  #vector form
            Phi(X) = [Phi(x_1), Phi(x_2), ..., Phi(x_N)].T              #matrix form


    Args:
        X (N-by-D numpy array): matrix of row-vector features (may also be a single row-vector)
        Omega (D-by-NFF numpy array): matrix of row-vector inner products
        B (NFF length numpy array): vector of translations



    Returns:
        A N-by-NFF numpy array matrix of transformed points, Phi(X)
    '''
    # return numpy.sqrt(1.0/Omega.shape[1])*numpy.cos(X.dot(Omega)+B)
    Phi = numpy.sqrt(1.0/Omega.shape[1])*numpy.cos(X.dot(Omega)+B)
    return numpy.concatenate((numpy.ones((X.shape[0], 1)), Phi), axis=1)

##################
# YOUR CODE HERE #
##################
def random_fourier_features(train_X, train_Y, num_fourier_features=100, alpha=0.05, num_iters=500, print_iters=False):
    '''
    Creates a random set of Fourier basis functions and fits a linear model in this space.

        Randomly sample num_fourier_features's non-linear transformations of the form:

            phi_k(x) = cos(<x,omega_k> + b_k)
            Phi(x) = sqrt(1/D)*[phi_1(x), phi_2(x), ..., phi_NFF(x)]

        where omega_k and b_k are sampled according to (Rahimi and Recht, 20018).


    Args:
        train_X (N-by-D numpy array): Training data features as a matrix of row vectors (train_X[i][j] is the jth component of the ith example)
        train_Y (length N numpy array): The training data target as a length N vector
        num_fourier_features (int): the number of random features to generate


    Returns:
        Theta (numpy array of length num_fourier_features+1): the weights for the *transformed* model
        Omega (D-by-num_fourier_features numpy array): the inner product term of the transformation
        B (numpy array of length num_fourier_features): the translation term of the transformation
    '''
    # You will find the following functions useful for sampling:
    # 	numpy.random.multivariate_normal() for normal random variables
    #	numpy.random.random() for Uniform random variables
    d = train_X.shape[1]
    cov = numpy.zeros((d, d))
    numpy.fill_diagonal(cov, 1)
    Omega = numpy.random.multivariate_normal(numpy.zeros(d), cov, num_fourier_features)
    Omega = Omega.T
    B = numpy.random.random(size=num_fourier_features) * (2 * numpy.pi)

    Phi = apply_RFF_transform(train_X,Omega,B)
    # here's an example of using numpy.random.random()
    # to generate a vector of length = (num_fourier_features), between -0.1 and 0.1
    initial_Theta = (numpy.random.random(size=(num_fourier_features+1,1))-0.5)*0.2
    step_history = linreg_grad_desc(initial_Theta,Phi,train_Y,alpha=alpha,num_iters=num_iters,print_iters=print_iters)
    print('loss for {} is {}'.format(num_fourier_features, step_history[-1][1]))
    return step_history[-1][0], Omega, B

def rff_model_sample(Theta,Omega,B,model_X):
    sampled_X = numpy.linspace(model_X.min(axis=0),model_X.max(axis=0),100)
    Phi = apply_RFF_transform(sampled_X,Omega,B)
    sampled_Y = Phi.dot(Theta)
    return sampled_X, sampled_Y

def vis_rff_model(train_X, train_Y, Theta, Omega, B):
    sample_X, sample_Y = rff_model_sample(Theta,Omega,B, train_X)
    plot_helper(train_X, train_Y, sample_X, sample_Y)


def print_lstsq(x, y, file_name):
    # print expected using inbuilt
    numpy.set_printoptions(linewidth=numpy.inf)
    lstsq_theta, sse, _, _ = numpy.linalg.lstsq(x, y, rcond=1)
    lstsq_loss = sse / (2 * y.shape[0])
    print('using linalg.lstsq for file {}, theta is {}, loss is {}'.format(file_name, lstsq_theta.tolist(), lstsq_loss))


def print_closed_form(data_X, data_Y, file_name, orig_data_X, plot=True, use3DPlot=False):
    print("input data shape is ", data_Y.shape)
    # closed form solution
    cf_theta = linreg_closed_form(data_X, data_Y)
    cf_loss = loss(cf_theta, data_X, data_Y)
    print('using closed form solution for file {}, theta is {}, loss is {}'.format(file_name, cf_theta.tolist(), cf_loss))
    # use 3d plot for 3d data
    if plot:
        vis_linreg_model_3d(orig_data_X, data_Y, cf_theta) if use3DPlot else vis_linreg_model(orig_data_X, data_Y, cf_theta)

def print_gradient_descent(data_X, data_Y, file_name='', alpha=0.05, num_iters=500, print_iters=False):
    # gradient descent
    init_theta = numpy.zeros([data_X.shape[1], 1])
    step_hist = linreg_grad_desc(init_theta, data_X, data_Y, alpha=alpha, num_iters=num_iters, print_iters=print_iters)
    gd_theta, gd_loss = step_hist[-1]
    print('For file {}, alpha - {}, num_iter- {}, gd theta - {}, loss - {}'.format(file_name, alpha, num_iters, gd_theta.tolist(), gd_loss))


def print_rff(file_name, small_K, medium_K, large_K, large_K_num_iters=500, large_K_alpha=0.05):
    print('***** processing file ', file_name)
    data_X, data_Y = load_data(file_name)
    # small K
    rff_theta, rff_omega, rff_b = random_fourier_features(data_X, data_Y, num_fourier_features=small_K)
    vis_rff_model(data_X, data_Y, rff_theta, rff_omega, rff_b)
    # medium K
    rff_theta, rff_omega, rff_b = random_fourier_features(data_X, data_Y, num_fourier_features=medium_K)
    vis_rff_model(data_X, data_Y, rff_theta, rff_omega, rff_b)
    # num_iters and alpha needed only for large K to get perfect fit
    rff_theta, rff_omega, rff_b = random_fourier_features(data_X, data_Y, num_fourier_features=large_K, num_iters=large_K_num_iters, alpha=large_K_alpha)
    vis_rff_model(data_X, data_Y, rff_theta, rff_omega, rff_b)


def load_data_add_ones(file_name):
    # for d-dimensions, adding a 1s feature to get it it d+1 for cases where the plane doesnt go through the origin
    # for example, the y values are the same for all x values, the fit is not good.
    data_X, data_Y = load_data(file_name)
    plot_helper(data_X, data_Y)
    one_arr = numpy.ones((data_X.shape[0], 1))
    return numpy.concatenate((one_arr, data_X), axis=1), data_Y, data_X

if __name__ == '__main__':

    # load data from closed form and gradient descent
    data_X_2d, data_Y_2d, orig_data_X_2d = load_data_add_ones('2D-noisy-lin.txt')
    data_X_1d, data_Y_1d, orig_data_X_1d = load_data_add_ones('1D-no-noise-lin.txt')

    # print lstsq to compare with closed form and gradient descent solution
    print_lstsq(data_X_2d, data_Y_2d, '2D-noisy-lin.txt')
    print_lstsq(data_X_1d, data_Y_1d, '1D-no-noise-lin.txt')

    # 1.a - fit a model to the data in 1D-no-noise-lin.txt and 2D-noisy-lin.txt
    print('********* closed form solution on 1D-no-noise-lin.txt and 2D-noisy-lin.txt ********')
    print_closed_form(data_X_1d, data_Y_1d, '1D-no-noise-lin.txt', orig_data_X_1d)
    print_closed_form(data_X_2d, data_Y_2d, '2D-noisy-lin.txt', orig_data_X_2d, use3DPlot=True)

    # 1.b - add duplicate feature
    print('\n********* closed form solution on duplicate column ********')
    data_X_2d_dup_col = numpy.hstack((data_X_2d, data_X_2d[:, [-1]]))
    try: #2D-noisy-lin.txt
        print_closed_form(data_X_2d_dup_col, data_Y_2d, '2D-noisy-lin.txt-dup-feature', orig_data_X_2d)
    except numpy.linalg.LinAlgError as e:
        print('2D-noisy-lin.txt - unable to inverse matrix because it is a ', e)

    data_X_1d_dup_col = numpy.hstack((data_X_1d, data_X_1d[:, [-1]]))
    try: #1D-no-noise-lin.txt
        print_closed_form(data_X_1d_dup_col, data_Y_1d, '1D-no-noise-lin.txt-dup-feature', orig_data_X_1d)
    except numpy.linalg.LinAlgError as e:
        print('1D-no-noise-lin.txt - unable to inverse matrix because it is a ', e)

    # 1.c - add duplicate row
    print('\n********* closed form solution on duplicate row ********')
    # 2D-noisy-lin.txt
    data_X_dup_row = numpy.vstack((data_X_2d, data_X_2d[-1, :]))
    data_Y_dup_row = numpy.vstack((data_Y_2d, data_Y_2d[-1, :]))
    print_closed_form(data_X_dup_row, data_Y_dup_row, '2D-noisy-lin.txt-dup-row', orig_data_X_2d, plot=False)
    # 1D-no-noise-lin.txt
    data_X_dup_row_1d = numpy.vstack((data_X_1d, data_X_1d[-1, :]))
    data_Y_dup_row_1d = numpy.vstack((data_Y_1d, data_Y_1d[-1, :]))
    print_closed_form(data_X_dup_row_1d, data_Y_dup_row_1d, '1D-no-noise-lin.txt-dup-row', orig_data_X_1d, plot=False)

    # 1.d - gradient descent on duplicate column and row
    print('\n********* gradient descent on duplicate column ********')
    # duplicate column - 1D-no-noise-lin.txt and 2D-noisy-lin.txt
    print_gradient_descent(data_X_1d_dup_col, data_Y_1d, '1D-no-noise-lin.txt-dup-col', alpha=0.03, num_iters=1000)
    print_gradient_descent(data_X_2d_dup_col, data_Y_2d, '2D-noisy-lin.txt-dup-col')
    print('\n********* gradient descent on duplicate row ********')
    # duplicate row - 1D-no-noise-lin.txt and 2D-noisy-lin.txt
    print_gradient_descent(data_X_dup_row_1d, data_Y_dup_row_1d, '1D-no-noise-lin.txt-dup-row')
    print_gradient_descent(data_X_dup_row, data_Y_dup_row, '2D-noisy-lin.txt-dup-row')

    # 2.a 1D-no-noise-lin.txt alpha=0.05, num iters=10, and initial Theta set to the zero vector.
    print('\n********* gradient descent on 1D-no-noise-lin.txt 10 iterations ********')
    print_gradient_descent(data_X_1d, data_Y_1d, alpha=0.05, num_iters=10, print_iters=True)
    # 2.b Using the default parameters for alpha and num iters, and with initial Theta set to 0
    print('\n********* gradient descent on default parameters ********')
    print_gradient_descent(data_X_1d, data_Y_1d)
    print_gradient_descent(data_X_2d, data_Y_2d)
    # 2.c values of the learning rate and iterations
    print('\n********* gradient descent learning rate and iterations ********')
    # for 1D-no-noise-lin.txt
    for alpha in [0.01, 0.05, 0.06, 0.065, 0.07, 0.1]:
        for num_iter in [1, 5, 10]:
            print_gradient_descent(data_X_1d, data_Y_1d, file_name='1D-no-noise-lin.txt', alpha=alpha, num_iters=num_iter)
    # for 2D-noise-lin.txt
    for alpha in [0.05, 0.1, 1.0, 1.2, 1.3, 1.5]:
        for num_iter in [1, 5, 10, 100, 500]:
            print_gradient_descent(data_X_2d, data_Y_2d, file_name='2D-noisy-lin.txt', alpha=alpha, num_iters=num_iter)

    # 3. Random Fourier Features
    print('\n********* Random Fourier Features ********')
    print_rff('1D-exp-samp.txt', 2, 50, 1000, large_K_num_iters=1000, large_K_alpha=0.2)
    print_rff('1D-exp-uni.txt', 5, 50, 1000, large_K_num_iters=1000, large_K_alpha=0.2)
    print_rff('1D-quad-uni.txt', 5, 100, 1000, large_K_num_iters=1000, large_K_alpha=0.3)
    print_rff('1D-quad-uni-noise.txt', 5, 100, 5000, large_K_num_iters=1000, large_K_alpha=0.3)

